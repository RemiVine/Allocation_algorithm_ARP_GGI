{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341d8a23",
   "metadata": {},
   "source": [
    " <h3 style=\"text-align: center;\"> Rémi Viné </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c599aa",
   "metadata": {},
   "source": [
    "<font size=5> _Construction of an open-source application of a simple allocation algorithm to assign students in groups_ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557357ab",
   "metadata": {},
   "source": [
    "# What for?\n",
    "\n",
    "Every year, the Graduate Institute of Geneva lauches its Applied Research Projects (ARP). It targets interdisciplinary master students and is an long term group project together with an institutional partner (International Organizations, Non-Governmental Organizations, companies, start-ups, etc.). There are a given number of pre-selected projects for a given number of students. [Details on ARP are to be found here.](https://www.graduateinstitute.ch/communications/news/invaluable-life-learning-applied-research-projects)\n",
    "\n",
    "# Constraints \n",
    "\n",
    "A minimum number of students is imposed (and a maximum too). All pre-selected projects must have students allocated. \n",
    "\n",
    "Before the allocation, students are asked to rank their preferences. They attribute their first-best preferences to a handful of projects, their second-best preferences to another set of projects, their third-best prefrences to another; lastly, the remaining projects are not part of the projects students considered interesting for them to work on. \n",
    "\n",
    "Ideally, all students are assigned their top preferences but, empirically, the allocation is rarely fully composed of first-best preferences. \n",
    "\n",
    "\n",
    "# Hungarian Algorithm (Kuhn, 1955)\n",
    "\n",
    "The present algorithm attemps to allocate students as well as possible, minimizing dissatisfaction. The most common and simple procedure used is the Hungarian algorithm and the current algothim is based on it. This is a linear optimization procedure where a cost matrice is minimized (or maximized). In practice, the procedure is relatively simple and consists in erasing step by step columns and rows. In the end, the selection starts with the first row(s) attributed with only one zero in all the columns.\n",
    "\n",
    "# In a nutshell\n",
    "\n",
    "This algorithm, entirely shown below with details, assumes that allocating students a non preferred choice is unreasonably \"expensive\" so that the minimization of the cost matrix dramatically suffers from allocating a student to a non-chosen project. Indeed, weights 1 are given to top choices, 2 to second best, 3 to third best, and __1000__ to non-chosen projects (!). Therefore, if the algorithm allocates a student to a non-chosen project, it is mechanically because no other students (or k students supposed to be assigned to this group) have selected this project. This implies that there might be a discretionary decision not to keep this project if not enough students selected this project among the selected preferences. \n",
    "\n",
    "Overall, this algorithm is very simple, transparent in its method and extremely fast to be implemented (the full code for one allocation, along with data cleaning and some summary statistics takes less than 3 seconds - on my laptop). Suffices to have an excel file ready with students' preference like the one of year 2022. If constructing a different data set, one simply needs to be careful in dropping the appropriate variables. Fundamentally, students' index and students' ranking for each project are the only necessary variables. The data inspection section might be edited depending on the excel data set loaded (and it is easy to drop proprietary excel towards .csv or any other data set formats). \n",
    "\n",
    "# Important drawback \n",
    "\n",
    "This is a specific algorithm well-suited for such allocation. However, the price to pay to an easy and efficient allocation is the lack of refinement such as the inclusion of heterogeneous characteristics on top of students' preferences. Here, there is no account for extra characteristics such as language match between the students' skills and the partners' needs. There is also no _ad hoc_ matching where some partners would have requested 5 students or, on the contrary, 2 students. Here all partners are treated equally and so are students' preferences. In some respect, this overly simple allocation has the advantage of fairness towards partners and students. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8bc2da",
   "metadata": {},
   "source": [
    "# Implement the Hungarian algorithm to the current context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dafb8d5",
   "metadata": {},
   "source": [
    "## Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128897fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from pulp import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65f43af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "#save_path = \"/home/remi/Dropbox/Other_tasks/Statistic_book/Graphs_and_pictures\"\n",
    "#data_load_path = \"/home/remi/Dropbox/Other_tasks/IHEID_ARP/2023/04_Algorithm/\"\n",
    "\n",
    "save_path = \"C:/Users/revine/OneDrive - EHL/Documents/IHEID/IHEID_ARP/2024/Allocation/\"\n",
    "data_load_path = \"C:/Users/revine/OneDrive - EHL/Documents/IHEID/IHEID_ARP/2024/Allocation\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29d792",
   "metadata": {},
   "source": [
    "The data used is simply the one used last year, for APR 2023. It appears that 254 students were registered and there were 70 projects. This implies that a few projects will have 3 students, and some others will have 4 students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa363725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (students in rows, projects in columns)\n",
    "#filename = \"MINT_ARP_2023_Students_preferences.xlsx\"\n",
    "#student_pref_data = pd.read_excel(os.path.join(data_load_path, filename), sheet_name= \"Prefs\")\n",
    "filename = \"ARP_results_coded_names.xlsx\"\n",
    "student_pref_data = pd.read_excel(os.path.join(data_load_path, filename), sheet_name= \"Form responses 2\")\n",
    "print(student_pref_data.shape) #dimension of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90153ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See data\n",
    "student_pref_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop not used variables\n",
    "#student_pref_data = student_pref_data.drop(student_pref_data.columns[[1,2,3,4,5,6,7]], axis= 1)\n",
    "student_pref_data = student_pref_data.drop(student_pref_data.columns[[0,2,3,74]], axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5df9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(student_pref_data.shape) #dimension of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8aef5d",
   "metadata": {},
   "source": [
    "Here, I assume that projects valued \"0\" are the preferred ones, \"3\" are allocated to the projects labelled \"extremely interested\", \"10\" are allocated to the projects deemed very interesting, \"20\" are allocated to the projects considered as interesting, \"50\" are the somewhat interested projects (among the ones selected by the student); last, projects valued \"10000\" are the non-chosen ones by the student. \n",
    "\n",
    "Replace zero-valued project by a very large number (arbitrarily set to a ten thousand here). The choice of the costs is arbirary but trying to make the least interesting projects for students unlikely to be matched to them, in order to reduce costs. It is perhaps the most important stage, as this boils down to the cost matrix. large costs in attributing a least preferred allocation can be very expensive. In the costs given here, assuming there are only four students, costs to give 2 students their favourite projects and two their very interesting projects is identical to costs to give 3 students their favourite projects and one the \"interesting\" project (costs of 20 in both cases). Allocating all 4 students their \"extremely interesting\" projects would be better (costs of 12). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for replacement\n",
    "mapping = {\n",
    "    \"Not interested\": 10000,   # 10000\n",
    "    \"Somewhat interested\": 50, # 50\n",
    "    \"Interested\": 20,          # 20\n",
    "    \"Very interested\": 10,     # 10\n",
    "    \"Extremely interested\": 3, # 3\n",
    "    \"Favourite\": 0             # 0\n",
    "}\n",
    "# Define the range of columns (from 1 to 70)\n",
    "start_column = 1  # Start column position\n",
    "end_column = 71   # End column position (inclusive)\n",
    "# Iterate through the selected columns by position and apply replacement\n",
    "for column_position in range(start_column, end_column + 1):\n",
    "    column_name = student_pref_data.columns[column_position - 1]  # Convert position to column name\n",
    "    student_pref_data[column_name] = student_pref_data[column_name].replace(mapping)\n",
    "# Display the output\n",
    "#print(student_pref_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487d7cb",
   "metadata": {},
   "source": [
    "## Define the different matrices and arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a variable\n",
    "student_pref_data = student_pref_data.rename(columns={'Email address': 'Etudiant'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98192cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the cost matrix from the table\n",
    "# Put the variable 'Etudiant' as index\n",
    "student_pref_data = student_pref_data.set_index('Etudiant')\n",
    "# Converting into numeric matrix\n",
    "cost_matrix = student_pref_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eaebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the cost matrix\n",
    "print(cost_matrix)\n",
    "cost_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows in the matrix\n",
    "num_rows = cost_matrix.shape[0]\n",
    "# Define a new array with values labeled as 1, 2, 3, ...\n",
    "number_students = np.arange(1, num_rows + 1)\n",
    "print(number_students)\n",
    "type(number_students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows in the matrix\n",
    "num_columns = cost_matrix.shape[1]\n",
    "# Define a new array with values labeled as 1, 2, 3, ...\n",
    "number_projects = np.arange(1, num_columns + 1)\n",
    "print(number_projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bf76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform numpy.ndarray into lists\n",
    "number_students = number_students.tolist()\n",
    "number_projects = number_projects.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f913362",
   "metadata": {},
   "source": [
    "Here, \"k\" is defined as the minimum number of students per groups so that each group has at least \"k\" students (symbol \"//\" is the floor division).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum number of students per project\n",
    "k = len(number_students) // len(number_projects)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad4f7a4",
   "metadata": {},
   "source": [
    "## Expand the cost matrix to make it square, simply duplicate the same initial matrix by factor k and add some columns if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0655e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extra array one would need to build if the number of students is not a perfect factor of the number of projects\n",
    "extra_column_array = np.full(len(number_students), 1000000)\n",
    "extra_column_array = np.expand_dims(extra_column_array, axis=1) # necessary for further stacking (when non-empty reminder of division above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129cc458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of extra expensive column to build\n",
    "extra_column = len(number_students) % len(number_projects)\n",
    "print(extra_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the idea is to expand the matrix by the number of people per task (source: https://stats.stackexchange.com/questions/232462/is-there-an-algorithm-for-solving-a-many-to-one-assignment-problem)\n",
    "# Stack columns k times\n",
    "stacked_matrix = np.tile(cost_matrix, (1, k))\n",
    "# Add extra columns if needed\n",
    "stacked_matrix_extra = np.tile(extra_column_array, (1, extra_column))\n",
    "# Put cost matrix duplicated and extra columns together\n",
    "if extra_column == 0:\n",
    "    cost_large = stacked_matrix\n",
    "else: \n",
    "    cost_large = np.hstack((stacked_matrix, stacked_matrix_extra))\n",
    "print(cost_large)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Linear Assignement problem\n",
    "prob = LpProblem(name = \"Assignment_Problem\", sense = LpMinimize) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa3dbc",
   "metadata": {},
   "source": [
    "### Define the decision variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf198354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cost data is made into a dictionary\n",
    "costs= makeDict([number_students, number_projects], cost_large, 0) # headers are workers & jobs, array is cost, and cost is by default 0\n",
    "# Creates a list of tuples containing all the possible assignments\n",
    "assign = [(s, p) for s in number_students for p in number_projects] # 4*4 number of tuples [(1,1), (1,2), ...)]\n",
    "# A dictionary called 'Vars' is created to contain the referenced variables\n",
    "vars = LpVariable.dicts(name = \"Assign\", indices = (number_students, number_projects), lowBound = 0, upBound = None, cat = LpBinary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efbd97",
   "metadata": {},
   "source": [
    "### Define the Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective function is added to 'prob' first\n",
    "prob += ( # starts the definition of a new term in the objective function.\n",
    "    lpSum([vars[s][p] * costs[s][p] for (s, p) in assign]),\n",
    "    \"Sum_of_Assignment_Costs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057749f",
   "metadata": {},
   "source": [
    "### Define the Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are row constraints. Each project can be assigned to only at most (k+1) students.\n",
    "for p in number_projects:\n",
    "    prob+= lpSum(vars[s][p] for s in number_students) <= k+1\n",
    "    prob+= lpSum(vars[s][p] for s in number_students) >= k\n",
    "# There are column constraints. Each student can be assigned to only one project.\n",
    "for s in number_students:\n",
    "    prob+= lpSum(vars[s][p] for p in number_projects) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe92b3b",
   "metadata": {},
   "source": [
    "### Solve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ad840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem is solved using PuLP's choice of Solver\n",
    "prob.solve()\n",
    "print(\"Value of Objective Function = \", value(prob.objective))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc036308",
   "metadata": {},
   "source": [
    "## Print all students-projects dyads & build a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the output strings\n",
    "output_list = []\n",
    "# Create an empty DataFrame\n",
    "df_allocation = pd.DataFrame(columns=[\"Student\", \"Group\"])\n",
    "# Print values equal to the target value\n",
    "for v in prob.variables():\n",
    "    if v.varValue == 1:\n",
    "        # Extract student and group numbers from the variable name\n",
    "        _, student, group = v.name.split(\"_\")\n",
    "        # Construct the output string\n",
    "        output = \"Student {} gets group {}\".format(student, group)\n",
    "        # Add the output string to the list\n",
    "        output_list.append(output)\n",
    "        # For dataframe\n",
    "        # Extract student and group information\n",
    "        student_info = int(student)\n",
    "        group_info = int(group)\n",
    "        # Add student and group information to the DataFrame (the row below is deprecated)\n",
    "        # df_allocation = df_allocation.append({\"Student\": student_info, \"Group\": group_info}, ignore_index=True)\n",
    "        # Create a new DataFrame for the new row\n",
    "        new_row = pd.DataFrame({\"Student\": [student_info], \"Group\": [group_info]})   \n",
    "        # Concatenate the new row with the original DataFrame\n",
    "        df_allocation = pd.concat([df_allocation, new_row], ignore_index=True)\n",
    "\n",
    "        # Print the DataFrame\n",
    "print(\"The dataframe is the following:\")\n",
    "print(df_allocation)          \n",
    "# Sort the output list by student numbers\n",
    "#output_list.sort(key=lambda x: int(x.split()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efecb42",
   "metadata": {},
   "source": [
    "## Produce some summary statistics regarding the allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allocation.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f922b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the allocation dataframe by Students' number\n",
    "df_allocation_sorted = df_allocation.sort_values('Student')\n",
    "df_allocation_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4cbab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reintroduce the variable \"Etudiant\" as a variable in the initial dataframe\n",
    "#student_pref_data['Etudiant'] = student_pref_data.index\n",
    "# Reset the index and add it as a new column\n",
    "#student_pref_data['Etudiant'] = student_pref_data.index\n",
    "# Extract the index and make it a new variable\n",
    "student_pref_data = student_pref_data.rename_axis('NewIndex')\n",
    "student_pref_data['Etudiant'] = student_pref_data.index\n",
    "student_pref_data['Student'] = student_pref_data.reset_index().index + 1\n",
    "student_pref_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all variables (columns) in the DataFrame\n",
    "variables = student_pref_data.columns.tolist()\n",
    "# Print the list of variables\n",
    "#print(\"Variables in the DataFrame:\")\n",
    "#for variable in variables:\n",
    "#    print(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for students' number in the initial data set\n",
    "#student_pref_data['Student'] = student_pref_data['Etudiant'].str.extract(r'- (\\d+)$')\n",
    "#student_pref_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c373185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the keys are of the same type, and integers.\n",
    "#student_pref_data['Student'] = student_pref_data['Student'].astype(int)\n",
    "#df_allocation_sorted['Etudiant'] = df_allocation_sorted.index\n",
    "df_allocation_sorted['Student'] = df_allocation_sorted['Student'].astype(int)\n",
    "df_allocation_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca16ee",
   "metadata": {},
   "source": [
    "### Merge the initial dataframe and the allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f19724",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = df_allocation_sorted.merge(student_pref_data, left_on='Student', right_on='Student')\n",
    "merged_data.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7967db",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_subset = merged_data[['Group', 'Etudiant']]\n",
    "# Group by 'Group' and combine the values in 'Language competencies for the ARP projects'\n",
    "merged_data_subset_grouped = merged_data_subset.groupby('Group', as_index=False).agg({\n",
    "    'Etudiant': ', '.join ,\n",
    "})\n",
    "# Split the column into multiple columns (as many as needed)\n",
    "merged_data_subset_grouped = merged_data_subset_grouped.join(merged_data_subset_grouped['Etudiant'].str.split(', ', expand=True))\n",
    "merged_data_subset_grouped = merged_data_subset_grouped.drop(columns=['Etudiant'])\n",
    "# Print the entire DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "#print(merged_data_subset_grouped.to_string(index=False))\n",
    "# Export in excel\n",
    "name_of_file = 'Student_list_per_project.xlsx'\n",
    "merged_data_subset_grouped.to_excel(save_path + name_of_file, index=False)  # Set index to False if you don't want to save the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change names of columns to correspond to group number\n",
    "# Rename columns 2 to the last projects' number\n",
    "for col_idx in range(2, len(number_projects)+2):\n",
    "    new_label = str(col_idx - 1)\n",
    "    old_name = merged_data.columns[col_idx]\n",
    "    merged_data = merged_data.rename(columns={old_name: new_label})\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734b3b3",
   "metadata": {},
   "source": [
    "### Verify frequency of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e25c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_number_student_group = merged_data['Group'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e5cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the Series into a DataFrame\n",
    "df_freq = frequency_number_student_group.to_frame()\n",
    "df_freq = df_freq.rename(columns={'Group': 'Frequency'})\n",
    "df_freq['Group'] = df_freq.index\n",
    "#print(df_freq)\n",
    "#df_group_size = df_freq[\"Frequency\"]\n",
    "# Grouping and counting frequencies\n",
    "group_size = df_freq['Frequency'].value_counts()\n",
    "# Converting the Series to a DataFrame\n",
    "df_group_size = group_size.reset_index()\n",
    "df_group_size.columns = ['Frequency', 'Count']\n",
    "# Creating a new variable by multiplying the index and Frequency\n",
    "df_group_size['Total Number Students'] = df_group_size['Frequency'] * df_group_size['Count']\n",
    "print(df_group_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee3eda",
   "metadata": {},
   "source": [
    "Visualize the share of groups with different number of students. The bar chart shows that there are only groups with three or four students, which is what was initially scheduled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating relative frequencies\n",
    "total_students = df_group_size['Total Number Students'].sum()\n",
    "print(\"Total number of students:\", total_students)\n",
    "df_group_size['Relative Frequency'] = df_group_size['Total Number Students'] / total_students\n",
    "print(df_group_size)\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(df_group_size['Frequency'], df_group_size['Relative Frequency'], color='skyblue')\n",
    "\n",
    "# Adding text labels on top of each bar for total number of students\n",
    "for i, v in enumerate(df_group_size['Total Number Students']):\n",
    "    plt.text(df_group_size['Frequency'][i], df_group_size['Relative Frequency'][i] + 0.01, str(v), ha='center')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title('Relative Frequencies with Total Number of Students')\n",
    "plt.xticks(df_group_size['Frequency'])\n",
    "# Save the plot to a file\n",
    "name_of_file = 'Bar_chart_proportion_students_by_group'\n",
    "completeName = os.path.join(save_path, name_of_file + \".png\")\n",
    "plt.savefig(completeName, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029a7c1",
   "metadata": {},
   "source": [
    "#### Find shares of students first first, second, third, fourth, no choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9091ac",
   "metadata": {},
   "source": [
    "Here, the allocated group is matched back to the students' choice. For example, if the student obtained his or her first choice, then the variable indicates \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Share_group'] = merged_data.apply(lambda row: row[str(row['Group'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute descriptive statistics\n",
    "stats = merged_data['Share_group'].describe()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from values to labels\n",
    "label_mapping = {\n",
    "    0: 'Favourite',\n",
    "    3: 'Extremely interested',\n",
    "    10: 'Very interested',\n",
    "    20: 'Interested',\n",
    "    50: 'Somewhat interested',\n",
    "    10000: 'Not interested'\n",
    "}\n",
    "# Map the 'Share_group' column to labels\n",
    "merged_data['Share_group'] = merged_data['Share_group'].map(label_mapping)\n",
    "# Count the frequency of each value\n",
    "frequency_share = merged_data['Share_group'].value_counts()\n",
    "frequency_share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454979e",
   "metadata": {},
   "source": [
    "Visualize the frequency of students obtaining their first, second and third choices. Beyond 80% of the students obtained their top choice, less than 1.4% of all students obtained a project in their third best set of projects. No student obtained a project that was not part of any preferred project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative frequencies\n",
    "relative_freq = frequency_share / len(merged_data)\n",
    "# Create the bar chart\n",
    "plt.bar(relative_freq.index, relative_freq, color='skyblue')\n",
    "# Set the labels and title\n",
    "plt.xlabel('Projects preferences', size=12)\n",
    "# Adjust the font size of the x-axis labels\n",
    "plt.xticks(fontsize=8)\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title(\"Bar Chart of Students' allocation by preference category\")\n",
    "# Add frequencies to the bars\n",
    "for i, freq in enumerate(frequency_share):\n",
    "    plt.text(i, relative_freq[i], f'{freq}', ha='center', va='bottom')\n",
    "# Save the plot to a file\n",
    "name_of_file = 'Bar_chart_proportion_satisfied_allocation'\n",
    "completeName = os.path.join(save_path, name_of_file + \".png\")\n",
    "plt.savefig(completeName, dpi=300, bbox_inches='tight')\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58809b42",
   "metadata": {},
   "source": [
    "# Check the language skills\n",
    "\n",
    "Importantly, students were asked about their language skills in order to ensure that the matching was also according to the language matching. Some partners informed on language requirements and/or on language preferred in the group (because of interviews to be conducted, literature only available in the local language, etc.). Unfortunately, this method cannot take such heterogeneity into account. That is the price of having a very simply and fully transparant analysis. \n",
    "\n",
    "However, one can argue that the self-selection of students will lead the allocation based on preference to allow for a sound language allocation on top of the students' preferences. It is expected that students must have internalized the language constraints into their preferences sorting. In fact, this is probably better to allocate using a simply algorithm and entrusting students so that language allocation is also appropriate. If not, it is not unrealistic to have the students bear the responsability of their own choices - as they were duly informed about the importance of the language allocation beforehand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset with students' email, langauge requests, and group allocation\n",
    "filename = \"ARP_results_coded_names.xlsx\"\n",
    "student_language= pd.read_excel(os.path.join(data_load_path, filename), sheet_name= \"Form responses 2\") \n",
    "# Specify the column numbers you want to keep (e.g., columns 0 and 2)\n",
    "columns_to_keep = [1, 74]\n",
    "# Use iloc to select columns by column numbers\n",
    "student_language = student_language.iloc[:, columns_to_keep]\n",
    "student_language.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stud_grouped_lang = student_language.merge(merged_data, left_on='Email address', right_on='Etudiant')\n",
    "stud_grouped_lang.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "stud_grouped_lang = stud_grouped_lang.iloc[:, [0,1,3]]\n",
    "stud_grouped_lang = stud_grouped_lang.sort_values('Group')\n",
    "stud_grouped_lang.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Language competencies for the ARP projects' to strings\n",
    "stud_grouped_lang['Language competencies for the ARP projects'] = stud_grouped_lang['Language competencies for the ARP projects'].astype(str)\n",
    "\n",
    "# Group by 'Group' and combine the values in 'Language competencies for the ARP projects'\n",
    "compressed_stud = stud_grouped_lang.groupby('Group', as_index=False).agg({\n",
    "    'Email address': ', '.join ,\n",
    "    'Language competencies for the ARP projects': ', '.join\n",
    "})\n",
    "compressed_stud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the column into multiple columns (as many as needed)\n",
    "compressed_stud = compressed_stud.join(compressed_stud['Language competencies for the ARP projects'].str.split(', ', expand=True))\n",
    "# Drop the original column because not needed\n",
    "compressed_stud = compressed_stud.drop(columns=['Language competencies for the ARP projects'])\n",
    "compressed_stud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f40909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset with students' email, langauge requests, and group allocation\n",
    "filename = \"ARP_2023_2024_allocation_constraints.xlsx\"\n",
    "detailed_skills= pd.read_excel(os.path.join(data_load_path, filename), sheet_name= \"Detailed_skills\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbbdcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_language = detailed_skills.merge(compressed_stud,left_on ='Project', right_on='Group')\n",
    "# Replace various representations of missing values with np.nan\n",
    "#merged_data_language = merged_data_language.replace('None', np.nan)\n",
    "merged_data_language = merged_data_language.replace({None: np.nan})\n",
    "merged_data_language = merged_data_language.replace(['nan'], np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f61e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_language.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f66aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_file = 'compare_language_allocation.xlsx'\n",
    "merged_data_language.to_excel(save_path + name_of_file, index=False)  # Set index to False if you don't want to save the index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd23653",
   "metadata": {},
   "source": [
    "### Example code to assign a coverage score of what are the criteria listed in a sequence of columns that are matched in another sequence of columns (availability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1801c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = {\n",
    "    'L1': ['Fr', 'Fr', 'Fr', 'Fr'],\n",
    "    'L2': ['Fr', 'Fr', 'Ge', 'Ge'],\n",
    "    'L3': ['Ge', 'Fr', np.nan, np.nan],\n",
    "    'L4': ['Fr', np.nan, np.nan, np.nan],\n",
    "    'M1': ['Sp', 'Fr', 'Ch', 'Fr'],\n",
    "    'M2': ['Fr', 'Ar', 'Ru', 'Ge'],\n",
    "    'M3': ['Fr', np.nan, np.nan, np.nan],\n",
    "    'M4': ['Fr', np.nan, np.nan, 'Fr'],\n",
    "    'M5': ['Ge', np.nan, 'Fr', 'Ge'],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to calculate the share of fully matched criteria for each row\n",
    "def calculate_match_share(row):\n",
    "    l_columns = row[['L1', 'L2', 'L3', 'L4']]\n",
    "    m_columns = row[['M1', 'M2', 'M3', 'M4', 'M5']]  \n",
    "    # Create a dictionary to store required criteria in L columns and their counts\n",
    "    l_criteria = {}\n",
    "    for l in l_columns:\n",
    "        if not pd.isna(l) and l != 'NONE':\n",
    "            if l not in l_criteria:\n",
    "                l_criteria[l] = 1\n",
    "            else:\n",
    "                l_criteria[l] += 1\n",
    "    if not l_criteria:\n",
    "        return np.nan\n",
    "    # Initialize a match count\n",
    "    match_count = 0\n",
    "    for l, count in l_criteria.items():\n",
    "        # Count how many times each string in L columns appears in M columns\n",
    "        count_in_m = m_columns.tolist().count(l)\n",
    "        # Update the match count based on the counts of strings in L and M columns\n",
    "        match_count += min(count, count_in_m)\n",
    "    # Calculate the match share\n",
    "    match_share = match_count / sum(l_criteria.values())\n",
    "    return match_share\n",
    "# Calculate the share of fully matched criteria for each row\n",
    "df['Match_Share'] = df.apply(calculate_match_share, axis=1)\n",
    "# Print the DataFrame with the match shares\n",
    "print(df[['Match_Share']])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the share of fully matched criteria for each row\n",
    "def calculate_match_share(row):\n",
    "    l_columns = row[['Language 1', 'Language 2', 'Language 3', 'Language 4', 'Language 5']]\n",
    "    #availability_columns = row[['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']]  \n",
    "    availability_columns = row[[0,1,2,3,4,5,6,7,8,9]]  \n",
    "    # Create a dictionary to store required criteria in L columns and their counts\n",
    "    l_criteria = {}\n",
    "    for l in l_columns:\n",
    "        if not pd.isna(l) and l != 'NONE':\n",
    "            if l not in l_criteria:\n",
    "                l_criteria[l] = 1\n",
    "            else:\n",
    "                l_criteria[l] += 1\n",
    "    if not l_criteria:\n",
    "        return np.nan\n",
    "    \n",
    "    # Initialize a match count\n",
    "    match_count = 0\n",
    "    for l, count in l_criteria.items():\n",
    "        # Count how many times each string in L columns appears in M columns\n",
    "        count_in_available = availability_columns.tolist().count(l)\n",
    "        # Update the match count based on the counts of strings in L and M columns\n",
    "        match_count += min(count, count_in_available)\n",
    "    # Calculate the match share\n",
    "    match_share = match_count / sum(l_criteria.values())\n",
    "    return match_share\n",
    "\n",
    "    ##############################################################################\n",
    "    ##############################################################################    \n",
    "def calculate_match_share_opt(row):\n",
    "    l_columns_options = row[['Optional language 1', 'Optional language 2', 'Optional language 3', 'Optional language 4', 'Optional language 5']]\n",
    "    #availability_columns = row[['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']]  \n",
    "    availability_columns = row[[0,1,2,3,4,5,6,7,8,9]]  \n",
    "    # Do the same for the optional criteria\n",
    "    l_criteria_opt = {}\n",
    "    for l in l_columns_options:\n",
    "        if not pd.isna(l) and l != 'NONE':\n",
    "            if l not in l_criteria_opt:\n",
    "                l_criteria_opt[l] = 1\n",
    "            else:\n",
    "                l_criteria_opt[l] += 1\n",
    "    if not l_criteria_opt:\n",
    "        return np.nan  \n",
    "    \n",
    "    # Initialize a match count\n",
    "    match_count_opt = 0\n",
    "    for l, count in l_criteria_opt.items():\n",
    "        # Count how many times each string in L columns appears in M columns\n",
    "        count_in_available = availability_columns.tolist().count(l)\n",
    "        # Update the match count based on the counts of strings in L and M columns\n",
    "        match_count_opt += min(count, count_in_available)\n",
    "    # Calculate the match share\n",
    "    match_share_opt = match_count_opt / sum(l_criteria_opt.values())\n",
    "    return match_share_opt   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32475b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_language.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfdb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the share of fully matched criteria for each row\n",
    "merged_data_language['Match_Share'] = merged_data_language.apply(calculate_match_share, axis=1)\n",
    "merged_data_language['Match_Share_Optional'] = merged_data_language.apply(calculate_match_share_opt, axis=1)\n",
    "# Print the DataFrame with the match shares\n",
    "#print(merged_data_language[['Match_Share', 'Match_Share_Optional', 'Group']])\n",
    "## Save exporting on excel\n",
    "name_of_file = 'ARP_2023_2024_matching_language_details.xlsx'\n",
    "merged_data_language.to_excel(save_path + name_of_file, index=False)  # Set index to False if you don't want to save the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341bb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "merged_data_language[['Match_Share', 'Match_Share_Optional']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30093e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for 'Age' and 'Income'\n",
    "plt.figure(figsize=(12, 4))\n",
    "#\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(merged_data_language['Match_Share'], \n",
    "         density=False, bins=10, edgecolor='black')\n",
    "plt.xlabel('Match_Share')\n",
    "plt.ylabel('Density')\n",
    "plt.title(\"Match Share between REQUIRED language and students' skills\")\n",
    "#\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(merged_data_language['Match_Share_Optional'], \n",
    "         density=False, bins=10, edgecolor='black', color='green')\n",
    "plt.xlabel('Match_Share_Optional')\n",
    "plt.ylabel('Density')\n",
    "plt.title(\"Match Share between OPTIONAL language and students' skills\")\n",
    "#\n",
    "plt.tight_layout()\n",
    "# Save the plot to a file\n",
    "name_of_file = 'Matching_shares_language_required_and_optional'\n",
    "completeName = os.path.join(save_path, name_of_file + \".png\")\n",
    "plt.savefig(completeName, dpi=300, bbox_inches='tight')\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop languages \n",
    "# Define the prefix to match\n",
    "#prefixes_to_drop = ['Language', 'Option']\n",
    "# Use list comprehensions to select columns that do not start with the prefix\n",
    "#columns_to_keep = [col for col in merged_data_language.columns if not any(isinstance(col, str) \n",
    "#                                                                          and col.startswith(prefix) \n",
    "#                                                                          for prefix in prefixes_to_drop)]\n",
    "# Create a new DataFrame with only the selected columns\n",
    "#merged_data_language = merged_data_language[columns_to_keep]\n",
    "merged_data_language = merged_data_language[['Group', 'Email address', 'Match_Share', 'Match_Share_Optional']]\n",
    "merged_data_language.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the column into multiple columns (as many as needed)\n",
    "merged_data_language = merged_data_language.join(merged_data_language['Email address'].str.split(', ', expand=True))\n",
    "# Drop the original column because not needed\n",
    "merged_data_language = merged_data_language.drop(columns=['Email address'])\n",
    "merged_data_language.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fd1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save exporting on excel\n",
    "name_of_file = 'ARP_2023_2024_matching_language_OVERALL.xlsx'\n",
    "merged_data_language.to_excel(save_path + name_of_file, index=False)  # Set index to False if you don't want to save the index\n",
    "#merged_data_language.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec23467",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Overall, using last year data, the result of this simple allocation is quite satisfactory, as most students obtain their first preference. It is even surprising because the self-selection of students allows for a large overlapping between students' language skills and the requests for the different projects.\n",
    "\n",
    "Another important asset of the current method is its pace to compute it. The code above took about two to three seconds (wall time) to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e151b15e",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "The main source used for the core procedure is [this webpage.](https://machinelearninggeek.com/solving-assignment-problem-using-linear-programming-in-python/)\n",
    " \n",
    "Kuhn, Harold W. \"The Hungarian method for the assignment problem.\" *Naval research logistics quarterly* 2, no. 1‐2 (1955): 83-97.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615892d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52270049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562a55a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
